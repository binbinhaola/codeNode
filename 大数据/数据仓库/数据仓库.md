<<<<<<< HEAD
## 数仓的理论介绍

### 为什么需要数据仓库

​    ![image-20230821114757198](F:\markdownImg\image-20230821114757198.png)

### 什么是数据仓库

![image-20230821114846337](F:\markdownImg\image-20230821114846337.png)

### 集成

 集成性是指数据仓库总的数据必须是一致的，数仓的数据是从原有的分散的多个数据库,数据文件和数据字段中抽取来的，数据来源既有内部数据也有外部数据

**集成方法**

  统一：消除不一致现象

  综合：对原有数据进行综合和计算

**非易失**

  数据仓库中的数据是经过抽取而形成的分析型数据。

  不具有原始性；执行的主要是'查询操作',一般情况下不执行'更新'操作

**随时间变化**

  时间维是数据仓库中很重要的一个维度

  不断新增数据，删除旧的数据，更新与时间有关的综合数据

**数据库和数仓的区别**

![image-20230822105654404](F:\markdownImg\image-20230822105654404.png)

**OLTP和OLAP**

![image-20230822110708116](F:\markdownImg\image-20230822110708116.png)



## Sqoop的安装

解压安装包

```shell
tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz 
```

配置全局环境变量

```shell
vim/etc/profile

#SQOOP_HOME
export SQOOP_HOME=/opt/sqoop
export PATH=$PATH:$SQOOP_HOME/bin
export HCAT_HOME=/opt/hive/hcatalog
export ACCUMULO_HOME=/opt/sqoop/src/java/org/apache/sqoop/accumulo

source /etc/profile
echo $SQOOP_HOME #查看目录
```

准备驱动

mysql-connector-java-8.0.17.jar; java-json.jar 至 /opt/sqoop/lib目录下

------

配置启动文件

```shell
cp /opt/sqoop/conf/sqoop-env-template.sh /opt/sqoop/conf/sqoop-env.sh --复制模板

export HADOOP_COMMON_HOME=/opt/hadoop #hadoop根目录
export HADOOP_MAPRED_HOME=/opt/hadoop #hadoop根目录
export HBASE_HOME=/opt/hbase  #hbase根目录
export HIVE_HOME=/opt/hive  #hive根目录
export ZOOCFGDIR=/opt/zookeeper #zookeeper根目录

sqoop help --验证sqoop是否配置成功
```



## **Sqoop的应用**

### 查询指令

```shell
sqoop list-databases #显示所有数据库
--connect jdbc:mysql://192.168.39.96:3306/ 
--username root 
--password 123456
#连接数据库

```

### 数据迁移

**mysql直接导入到hive表**

```shell
#!/bin/bash

DATABASE_URL="jdbc:mysql://localhost:3306/sales_source?serverTimezone=GMT%2B8"
USERNAME="root"
PASSWORD="123456"
TABLE="customer"
TARGET_DIR="/user/hive/warehouse"

sqoop import \
--connect $DATABASE_URL \
--driver com.mysql.jdbc.Driver \
--username $USERNAME \
--password $PASSWORD \
--table $TABLE \
--hive-import \
--hive-database sales_ods \
--hive-table ods_customer \
--hive-partition-key dt \
--hive-partition-value 202308241530 \
--target-dir $TARGET_DIR/dt=202308241530 \
-m 1

```



**mysql导出数据至hdfs,再将数据导入到hive中**

**mysql导出的为hive表的数据格式，实际上为文本文件，hive设置好分隔格式后可以直接导入**

```shell
#!/bin/bash

DATABASE_URL="jdbc:mysql://node1:3306/sales_source?serverTimezone=GMT%2B8"
USERNAME="root"
PASSWORD="123456"
DRIVER="com.mysql.jdbc.Driver "
TABLE="product"
TARGET_DIR="/tmp/sales/ods/product"

sqoop import \
--connect $DATABASE_URL \
--driver $DRIVER \
--table $TABLE \
--username $USERNAME \
--password $PASSWORD \
--m 1 \
--target-dir $TARGET_DIR

#创建hive表
create table sales_ods.ods_product (
    product_code int,
    product_name string,
    prodcut_category string
)
row format delimited fields terminated by ','

load data inpath '/tmp/sales/ods/product/part-m-00000' into table sales_ods.ods_product  #导入数据
```



**job任务的编写**

```shell
#!/bin/bash

JOB_NAME="mysql_sales_source_to_hive_append_job"
DATABASE_URL="jdbc:mysql://node1:3306/sales_source?serverTimezone=GMT%2B8"
USERNAME="root"
PASSWORD="123456"
DRIVER="com.mysql.cj.jdbc.Driver"
TABLE="sales_order"
HIVE_DATABASE="sales_ods"
HIVE_TABLE="ods_orders_partition"


sqoop job \
--create $JOB_NAME \
-- import \
--connect $DATABASE_URL \
--driver $DRIVER \
--username $USERNAME \
--password $PASSWORD \
--table $TABLE \
--incremental append \
--check-column order_number \
--last-value '0' \
--hive-import \
--hive-database $HIVE_DATABASE \
--hive-table $HIVE_TABLE \
--m 1

#!/bin/bash

JOB_NAME="mysql_sales_source_to_hive_append_job"

sqoop job --show $JOB_NAME
INCREMENTAL_VALUE=$(awk '/incremental.last.value/ {print $NF}' /root/.sqoop/metastore.db.script)
sqoop job -exec $JOB_NAME

```

=======
## 数仓的理论介绍

### 为什么需要数据仓库

​    ![image-20230821114757198](F:\markdownImg\image-20230821114757198.png)

### 什么是数据仓库

![image-20230821114846337](F:\markdownImg\image-20230821114846337.png)

### 集成

 集成性是指数据仓库总的数据必须是一致的，数仓的数据是从原有的分散的多个数据库,数据文件和数据字段中抽取来的，数据来源既有内部数据也有外部数据

**集成方法**

  统一：消除不一致现象

  综合：对原有数据进行综合和计算

**非易失**

  数据仓库中的数据是经过抽取而形成的分析型数据。

  不具有原始性；执行的主要是'查询操作',一般情况下不执行'更新'操作

**随时间变化**

  时间维是数据仓库中很重要的一个维度

  不断新增数据，删除旧的数据，更新与时间有关的综合数据

**数据库和数仓的区别**

![image-20230822105654404](F:\markdownImg\image-20230822105654404.png)

**OLTP和OLAP**

![image-20230822110708116](F:\markdownImg\image-20230822110708116.png)



## Sqoop的安装

解压安装包

```shell
tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz 
```

配置全局环境变量

```shell
vim/etc/profile

#SQOOP_HOME
export SQOOP_HOME=/opt/sqoop
export PATH=$PATH:$SQOOP_HOME/bin
export HCAT_HOME=/opt/hive/hcatalog
export ACCUMULO_HOME=/opt/sqoop/src/java/org/apache/sqoop/accumulo

source /etc/profile
echo $SQOOP_HOME #查看目录
```

准备驱动

mysql-connector-java-8.0.17.jar; java-json.jar 至 /opt/sqoop/lib目录下

------

配置启动文件

```shell
cp /opt/sqoop/conf/sqoop-env-template.sh /opt/sqoop/conf/sqoop-env.sh --复制模板

export HADOOP_COMMON_HOME=/opt/hadoop #hadoop根目录
export HADOOP_MAPRED_HOME=/opt/hadoop #hadoop根目录
export HBASE_HOME=/opt/hbase  #hbase根目录
export HIVE_HOME=/opt/hive  #hive根目录
export ZOOCFGDIR=/opt/zookeeper #zookeeper根目录

sqoop help --验证sqoop是否配置成功
```



## **Sqoop的应用**

### 查询指令

```shell
sqoop list-databases #显示所有数据库
--connect jdbc:mysql://192.168.39.96:3306/ 
--username root 
--password 123456
#连接数据库

```

### 数据迁移

**mysql直接导入到hive表**

```shell
#!/bin/bash

DATABASE_URL="jdbc:mysql://localhost:3306/sales_source?serverTimezone=GMT%2B8"
USERNAME="root"
PASSWORD="123456"
TABLE="customer"
TARGET_DIR="/user/hive/warehouse"

sqoop import \
--connect $DATABASE_URL \
--driver com.mysql.jdbc.Driver \
--username $USERNAME \
--password $PASSWORD \
--table $TABLE \
--hive-import \
--hive-database sales_ods \
--hive-table ods_customer \
--hive-partition-key dt \
--hive-partition-value 202308241530 \
--target-dir $TARGET_DIR/dt=202308241530 \
-m 1

```



**mysql导出数据至hdfs,再将数据导入到hive中**

**mysql导出的为hive表的数据格式，实际上为文本文件，hive设置好分隔格式后可以直接导入**

```shell
#!/bin/bash

DATABASE_URL="jdbc:mysql://node1:3306/sales_source?serverTimezone=GMT%2B8"
USERNAME="root"
PASSWORD="123456"
DRIVER="com.mysql.jdbc.Driver "
TABLE="product"
TARGET_DIR="/tmp/sales/ods/product"

sqoop import \
--connect $DATABASE_URL \
--driver $DRIVER \
--table $TABLE \
--username $USERNAME \
--password $PASSWORD \
--m 1 \
--target-dir $TARGET_DIR

#创建hive表
create table sales_ods.ods_product (
    product_code int,
    product_name string,
    prodcut_category string
)
row format delimited fields terminated by ','

load data inpath '/tmp/sales/ods/product/part-m-00000' into table sales_ods.ods_product  #导入数据
```



**job任务的编写**

```shell
#!/bin/bash

JOB_NAME="mysql_sales_source_to_hive_append_job"
DATABASE_URL="jdbc:mysql://node1:3306/sales_source?serverTimezone=GMT%2B8"
USERNAME="root"
PASSWORD="123456"
DRIVER="com.mysql.cj.jdbc.Driver"
TABLE="sales_order"
HIVE_DATABASE="sales_ods"
HIVE_TABLE="ods_orders_partition"


sqoop job \
--create $JOB_NAME \
-- import \
--connect $DATABASE_URL \
--driver $DRIVER \
--username $USERNAME \
--password $PASSWORD \
--table $TABLE \
--incremental append \
--check-column order_number \
--last-value '0' \
--hive-import \
--hive-database $HIVE_DATABASE \
--hive-table $HIVE_TABLE \
--m 1

#!/bin/bash

JOB_NAME="mysql_sales_source_to_hive_append_job"

sqoop job --show $JOB_NAME
INCREMENTAL_VALUE=$(awk '/incremental.last.value/ {print $NF}' /root/.sqoop/metastore.db.script)
sqoop job -exec $JOB_NAME

```

>>>>>>> e6f9c7032619ea760d965be70abd78cafff1d124
